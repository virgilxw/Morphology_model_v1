{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from random import sample\n",
    "from numpy.random import uniform\n",
    "from math import isnan\n",
    "from sklearn.preprocessing import scale\n",
    "import geopandas as gpd\n",
    "import contextily as ctx \n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "local_crs = 27700\n",
    "place = \"test\"\n",
    "lat = 55.86421405612109\n",
    "lng = -4.251846930489373\n",
    "country = \"UK\"\n",
    "crs=4326\n",
    "radius=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = gpd.read_parquet(f\"../output/{place}/p5-grid-output.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.set_index(['row', 'col'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_col = grid[['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsubset = grid.head().columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dropped = grid.fillna(0)\n",
    "grid_dropped = grid_dropped[grid_dropped['building_count_x'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of substrings to check in the column names\n",
    "substrings = ['index', 'bID']\n",
    "\n",
    "# Identify columns to drop\n",
    "columns_to_drop = [col for col in grid_dropped.columns if any(substring in col for substring in substrings)]\n",
    "\n",
    "# Drop the identified columns\n",
    "grid_dropped = grid_dropped.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(grid_dropped.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dropped_old = grid_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dropped[\"index\"] = grid_dropped.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dropped_geo = grid_dropped\n",
    "grid_dropped = grid_dropped.drop(columns=[\"geometry\", \"index\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(svd_solver='randomized', random_state=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "tessellation_scaled = scaler.fit_transform(grid_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(tessellation_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = grid_dropped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Ratio\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Ratio bar plot for each PCA components.\n",
    "plt.figure(figsize = (10, 5))\n",
    "ax = plt.bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\n",
    "plt.xlabel(\"PCA Components\",fontweight = 'bold')\n",
    "plt.ylabel(\"Variance Ratio\",fontweight = 'bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cumulative sum of explained variance ratios\n",
    "cumulative_sum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "org_col = list(grid_dropped.columns)\n",
    "\n",
    "num_pc = np.argmax(cumulative_sum >= 0.8) + 1\n",
    "\n",
    "pc_dict = {'Attribute': org_col}\n",
    "\n",
    "pc_dict.update({f'PC_{i+1}':pca.components_[i] for i in range(num_pc)})\n",
    "\n",
    "attributes_pca = pd.DataFrame(pc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot to visualize the Cumulative variance against the Number of components\n",
    "\n",
    "fig = plt.figure(figsize = (12,5))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.vlines(x=num_pc, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\n",
    "plt.xlabel('Number of PCA components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the dataframe using Incremental PCA for better efficiency.\n",
    "\n",
    "inc_pca = IncrementalPCA(n_components=num_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the scaled df on incremental pca\n",
    "\n",
    "df_inc_pca = inc_pca.fit_transform(tessellation_scaled)\n",
    "df_inc_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_df = pd.DataFrame(inc_pca.components_, columns=features)  # Adjust index based on n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the provided code with modifications to output to a text file\n",
    "\n",
    "# Creating a string to capture the text that will be printed, for writing to a file\n",
    "output_text = \"\"\n",
    "\n",
    "# Assuming components_df is already defined as shown previously\n",
    "number_of_features_to_describe = 15  # Change this to choose how many top features to describe for each component\n",
    "\n",
    "# Iterating through each principal component\n",
    "for component in components_df.index:\n",
    "    component_description = f\"Describing {component}:\\n\"\n",
    "    output_text += component_description\n",
    "    \n",
    "    # Sorting the features by their contribution to the component\n",
    "    sorted_features = components_df.loc[component].abs().sort_values(ascending=False)\n",
    "    \n",
    "    # Picking the top features\n",
    "    top_features = sorted_features.head(number_of_features_to_describe).index\n",
    "    contributions = sorted_features.head(number_of_features_to_describe).values\n",
    "    \n",
    "    # Printing out the top features and their contributions\n",
    "    for feature, contribution in zip(top_features, contributions):\n",
    "        feature_description = f\" - {feature} with a loading of {contribution:.2f}\\n\"\n",
    "        output_text += feature_description\n",
    "    \n",
    "    output_text += \"\\n\"  # Add a new line for better readability\n",
    "\n",
    "print(output_text)\n",
    "# Writing the output to a text file\n",
    "with open(f\"../output/{place}/PC_summary.txt\", \"w\") as file:\n",
    "    file.write(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"../output/{place}/pca.pickle\", 'wb') as f:\n",
    "    pickle.dump(components_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new dataframe with Principal components\n",
    "\n",
    "\n",
    "df_pca = pd.DataFrame(df_inc_pca, columns=[f\"PC_{i+1}\" for i in range(num_pc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca[\"row\"] = grid_dropped[\"row\"]\n",
    "df_pca[\"col\"] = grid_dropped[\"col\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(grid_dropped) == len(df_pca):\n",
    "    df_pca.index = grid_dropped.index\n",
    "else:\n",
    "    print(\"Error: The number of rows in grid_dropped and df_pca do not match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_col.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_geometry = geo_col.reset_index().merge(df_pca, on=[\"row\", \"col\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_geometry = df_pca_geometry.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = df_pca_geometry.drop(columns=[\"row\", \"col\", \"geometry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_plot = df_pca_geometry.dropna().columns.drop('geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows and columns for the subplot grid\n",
    "n_cols = 2  # You can adjust this based on your preference\n",
    "n_rows = (len(columns_to_plot) + 1) // n_cols\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "\n",
    "# Flatten axes array for easy iteration, if there's more than one row\n",
    "axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "\n",
    "# Loop through the columns and create a plot for each\n",
    "for i, column in enumerate(columns_to_plot):\n",
    "    # Plot with translucent colors\n",
    "    df_pca_geometry.dropna().plot(column=column, scheme=\"natural_breaks\", ax=axes[i], legend=True, alpha=0.5)  # Adjust alpha for translucency\n",
    "\n",
    "    # Add Contextily basemap\n",
    "    ctx.add_basemap(axes[i], crs=df_pca_geometry.crs.to_string())\n",
    "\n",
    "    axes[i].set_title(column)\n",
    "    axes[i].set_axis_off()\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_geometry.to_parquet(f\"../output/{place}/df_pca_geom.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"../output/{place}/df_pca_geometry.pickle\", 'wb') as f:\n",
    "    pickle.dump(df_pca_geometry, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_with_uID = df_pca.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_with_uID[\"uID\"] = [str(i) for i in grid_dropped.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating Hopkins score to know whether the data is good for clustering or not.\n",
    "\n",
    "# def hopkins(X):\n",
    "#     d = X.shape[1]\n",
    "#     n = len(X)\n",
    "#     m = int(0.1 * n) \n",
    "#     nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n",
    " \n",
    "#     rand_X = sample(range(0, n, 1), m)\n",
    " \n",
    "#     ujd = []\n",
    "#     wjd = []\n",
    "#     for j in range(0, m):\n",
    "#         u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n",
    "#         ujd.append(u_dist[0][1])\n",
    "#         w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n",
    "#         wjd.append(w_dist[0][1])\n",
    " \n",
    "#     HS = sum(ujd) / (sum(ujd) + sum(wjd))\n",
    "#     if isnan(HS):\n",
    "#         print(ujd, wjd)\n",
    "#         HS = 0\n",
    " \n",
    "#     return HS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hopkins score\n",
    "# Hopkins_score=round(hopkins(df_pca),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Hopkins_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hopkins statistic (introduced by Brian Hopkins and John Gordon Skellam) is a way of measuring the cluster tendency of a data set.[1] It belongs to the family of sparse sampling tests. It acts as a statistical hypothesis test where the null hypothesis is that the data is generated by a Poisson point process and are thus uniformly randomly distributed.[2] A value close to 1 tends to indicate the data is highly clustered, random data will tend to result in values around 0.5, and uniformly distributed data will tend to result in values close to 0.[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_pca_with_uID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.to_parquet(f\"../output/{place}/df_pca.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "processor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
