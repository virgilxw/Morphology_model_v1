{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from clustergram import Clustergram\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "local_crs = 27700\n",
    "place = \"test_OS\"\n",
    "lat = 55.86421405612109\n",
    "lng = -4.251846930489373\n",
    "country = \"UK\"\n",
    "crs=4326\n",
    "radius=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_crs = 27700\n",
    "place = \"Glasgow_OS\"\n",
    "lat = 55.86421405612109\n",
    "lng = -4.251846930489373\n",
    "country = \"UK\"\n",
    "crs=4326\n",
    "radius=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = f\"../output/{place}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate file names based on the naming pattern\n",
    "file_names = [os.path.join(output_directory, f\"percentile_chunk_{i}.pq\") for i in range(84)]\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over the generated file names and read each file\n",
    "for file_name in file_names:\n",
    "    if os.path.exists(file_name):\n",
    "        try:\n",
    "            df = pd.read_parquet(file_name)\n",
    "            dataframes.append(df)\n",
    "            print(f\"Successfully read {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file_name}. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_name}\")\n",
    "\n",
    "# Concatenate all the dataframes into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop(columns=[\"height_25\", \"height_50\", \"height_75\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized = (combined_df - combined_df.mean()) / combined_df.std()\n",
    "standardized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgram = Clustergram(range(1, 30))\n",
    "cgram.fit(standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgram.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgram.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgram.labels.to_parquet(f\"../output/{place}/cgram_labels.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(10, 10), sharex=True)\n",
    "cgram.calinski_harabasz_score().plot(xlabel=\"Number of clusters (k)\", ylabel=\"Calinski-Harabasz score\", ax=axs[1])\n",
    "cgram.davies_bouldin_score().plot(xlabel=\"Number of clusters (k)\", ylabel=\"davies_bouldin_score\", ax=axs[0])\n",
    "sns.despine(offset=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tessellation = gpd.read_parquet(f\"../output/{place}/tessellation_morphometric_p3.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized[\"cluster\"] = cgram.labels[num_clusters].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized[\"geometry\"] = tessellation[\"geometry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_cluster = gpd.GeoDataFrame(standardized, geometry=\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_array = np.mean(cgram.cluster_centers[num_clusters], axis=1)\n",
    "weighted_difference_between_clusters = {i: k for i, k, in enumerate(reduced_array)}\n",
    "\n",
    "def scale_dict(d):\n",
    "    # Extract values and convert them to a numpy array\n",
    "    values = np.array(list(d.values()))\n",
    "\n",
    "    # Normalize values to [0,1]\n",
    "    normalized_values = (values - np.min(values)) / (np.max(values) - np.min(values))\n",
    "\n",
    "    # Scale values from [-10,10]\n",
    "    scaled_values = (normalized_values * 20) - 10\n",
    "\n",
    "    # Create a new dictionary with the scaled values\n",
    "    scaled_dict = {key: value for key, value in zip(d.keys(), scaled_values)}\n",
    "\n",
    "    return scaled_dict\n",
    "\n",
    "weighted_difference_between_clusters = scale_dict(weighted_difference_between_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_clusters = sorted(weighted_difference_between_clusters.items(), key=lambda x: x[1])\n",
    "# Sort the dictionary by values in ascending order\n",
    "sorted_clusters = sorted(weighted_difference_between_clusters.items(), key=lambda x: x[1])\n",
    "\n",
    "# Create a mapping of current column names to new column names\n",
    "column_mapping = {cluster_id: index + 1 for index, (cluster_id, _) in enumerate(sorted_clusters)}\n",
    "weighted_difference_between_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_cluster[\"cluster_ID\"].map(column_mapping)\n",
    "\n",
    "tess_cluster[\"cluster_ID\"] = tess_cluster[\"cluster_ID\"].map(column_mapping)\n",
    "\n",
    "def rename_dictionary_keys(original_dict, rename_dict):\n",
    "    renamed_dict = {}\n",
    "    for key, value in original_dict.items():\n",
    "        if key in rename_dict:\n",
    "            new_key = rename_dict[key]\n",
    "        else:\n",
    "            new_key = key\n",
    "        renamed_dict[new_key] = value\n",
    "    return renamed_dict\n",
    "\n",
    "renamed_dict = rename_dictionary_keys(weighted_difference_between_clusters, column_mapping)\n",
    "weighted_difference_between_clusters = renamed_dict\n",
    "\n",
    "tess_cluster[\"one_dimensional_diff_between_clusters\"] = tess_cluster[\"cluster_ID\"].map(renamed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_cluster[\"one_dimensional_diff_between_clusters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import contextily as ctx\n",
    "import glasbey\n",
    "\n",
    "# Assuming tess_cluster and df_pca_geometry are predefined GeoDataFrames\n",
    "\n",
    "# Plotting the 'tess_cluster' with 'cluster' as a categorical variable\n",
    "fig, ax = plt.subplots(figsize=(300, 300))  # Adjust the figure size as needed\n",
    "\n",
    "# Generate a colormap with 30 distinct pastel colors\n",
    "palette = glasbey.extend_palette(\"tab20\", palette_size=num_clusters)\n",
    "\n",
    "cmap = mcolors.ListedColormap(palette)\n",
    "\n",
    "tess_cluster.plot(column='cluster', categorical=True, ax=ax, legend=True, cmap=cmap)\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Adding labels to each cell\n",
    "for idx, row in tess_cluster.iterrows():\n",
    "    # Getting the centroid of each geometry\n",
    "    centroid = row.geometry.centroid\n",
    "    # Annotating with the cluster number\n",
    "    ax.annotate(text=row['cluster'], xy=(centroid.x, centroid.y),\n",
    "                ha='center', va='center', fontsize=8, color='black')\n",
    "\n",
    "# Adding a basemap to the plot\n",
    "# Ensure that the CRS of df_pca_geometry is compatible with contextily basemaps\n",
    "ctx.add_basemap(ax, crs=tess_cluster.crs.to_string())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_cluster.to_parquet(f\"../output/{place}/p6_tess_cluster_out.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_cluster = gpd.read_parquet(f\"../output/{place}/p6_tess_cluster_out.pq\")\n",
    "tess_cluster = tess_cluster.drop(columns=[\"cluster_ID\", \"geometry\", \"one_dimensional_diff_between_clusters\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_cluster.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming tess_cluster is your DataFrame from the previous step\n",
    "\n",
    "variable_names = tess_cluster.columns[~tess_cluster.columns.isin(['geometry', 'cluster'])]\n",
    "\n",
    "# Assuming tess_cluster is your DataFrame from the previous step\n",
    "# and variable_names is a list of your variable names\n",
    "\n",
    "# Calculate the median for each variable across the entire dataset\n",
    "overall_median = tess_cluster[variable_names].median()\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "furthest_from_median = {}\n",
    "\n",
    "# Iterate over each cluster\n",
    "for cluster_id in tess_cluster['cluster'].unique():\n",
    "    # Calculate the mean of each variable in the cluster\n",
    "    cluster_mean = tess_cluster[tess_cluster['cluster'] == cluster_id][variable_names].mean()\n",
    "    \n",
    "    # Calculate the absolute difference from the overall median\n",
    "    difference_from_median = abs(cluster_mean - overall_median)\n",
    "    \"\"\n",
    "    # Sort the differences and get the top 10\n",
    "    top_10_variables = difference_from_median.sort_values(ascending=False).head(10)\n",
    "    \n",
    "    # Store the results in the dictionary\n",
    "    furthest_from_median[cluster_id] = top_10_variables\n",
    "\n",
    "sorted_furthest_from_median = sorted(furthest_from_median.items(), key=lambda item: item[0])\n",
    "\n",
    "with open(f'../output/{place}/cluster_analysis.txt', 'w') as file:\n",
    "    # Write the results to the file\n",
    "    for key in sorted(list(furthest_from_median.keys())):\n",
    "        file.write(f\"Cluster {key}:\\n\")\n",
    "        file.write(furthest_from_median[key].to_string())\n",
    "        file.write(\"\\n\\n\")\n",
    "\n",
    "print(\"Analysis saved to 'cluster_analysis.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming tess_cluster is a GeoDataFrame\n",
    "# Replace 'your_data.geojson' with the actual file or use your loaded GeoDataFrame\n",
    "\n",
    "# Assuming 'cluster' is the column with k-means clustering results\n",
    "\n",
    "# Get variable names from columns\n",
    "variable_names = tess_cluster.columns[~tess_cluster.columns.isin(['geometry', 'cluster'])]\n",
    "\n",
    "# Create a DataFrame to store the scaled values\n",
    "tess_cluster = pd.DataFrame()\n",
    "\n",
    "# Min-Max scaling each variable within each cluster\n",
    "for cluster_id in tess_cluster['cluster'].unique():\n",
    "    cluster_data = tess_cluster[tess_cluster['cluster'] == cluster_id][variable_names]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_cluster = scaler.fit_transform(cluster_data)\n",
    "    scaled_cluster_df = pd.DataFrame(scaled_cluster, columns=variable_names)\n",
    "    scaled_cluster_df['cluster'] = cluster_id\n",
    "    tess_cluster = pd.concat([tess_cluster, scaled_cluster_df], ignore_index=True)\n",
    "\n",
    "# Create a square heatmap\n",
    "plt.figure(figsize=(150, 150))\n",
    "sns.heatmap(tess_cluster.groupby('cluster')[variable_names].mean().T, cmap='coolwarm', annot=True, fmt=\".2f\", cbar_kws={'label': 'Scaled Value'}, square=True)\n",
    "plt.title('Scaled Values of Variables in Each Cluster (Min-Max Scaling)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_cluster = gpd.read_parquet(f\"../output/{place}/p6_tess_cluster_out.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_cluster = tess_cluster.drop(columns=[\"one_dimensional_diff_between_clusters\", \"geometry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = tess_cluster.groupby('cluster_ID')\n",
    "\n",
    "# Calculate the mean of each group (excluding the cluster_id column)\n",
    "centroids = grouped.mean()\n",
    "\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Assuming X_scaled_df is your scaled DataFrame and num_clusters is defined\n",
    "sl_mergings = linkage(centroids, method=\"single\", metric='euclidean')\n",
    "\n",
    "# Create the dendrogram\n",
    "dendrogram(sl_mergings)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
